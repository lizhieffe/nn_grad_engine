{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcntstihNX/ufe/IPKn/F2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhieffe/nn_grad_engine/blob/main/Auto_Differentiator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tutorial: https://bclarkson-code.com/posts/llm-from-scratch-scalar-autograd/post.html"
      ],
      "metadata": {
        "id": "3T6Os1kHWdaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional"
      ],
      "metadata": {
        "id": "9DGdsqzFYcGD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLDWUaemVPbp",
        "outputId": "550663eb-086f-4927-ea4a-5fdc4fd30af8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tensor(5)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "class Tensor:\n",
        "\n",
        "  value: float\n",
        "\n",
        "  # If the tensor is the result of an operation, the operation arguments are\n",
        "  # stored in the args.\n",
        "  args: tuple[\"Tensor\"] = ()\n",
        "\n",
        "  # If the Tensor is the result of an operation, the derivatives to the\n",
        "  # operation arguments are stored in local_derivatives.\n",
        "  #\n",
        "  # length == # of the input Tensors\n",
        "  local_derivatives: tuple[\"Tensor\"] = ()\n",
        "\n",
        "  # The derivative of the loss to the current Tensor in back prop.\n",
        "  derivative: Optional[\"Tensor\"] = None\n",
        "\n",
        "  def __init__(self, value: float) -> None:\n",
        "    self.value = value\n",
        "\n",
        "  def __repr__(self) -> str:\n",
        "    return f\"Tensor({self.value})\"\n",
        "\n",
        "\n",
        "\n",
        "  def backward(self) -> None:\n",
        "    # if not self.args:\n",
        "    #   return\n",
        "\n",
        "    # if not self.args:\n",
        "    #   raise Exception(\"Tensor has no args\")\n",
        "\n",
        "    # if not self.local_derivatives:\n",
        "    #   raise Exception(\"Tensor has no local derivatives\")\n",
        "\n",
        "    # if not self.derivative:\n",
        "    #   self.derivative = Tensor(1)\n",
        "\n",
        "    # for arg, local_derivative in zip(self.args, self.local_derivatives):\n",
        "    #   d = _mul(local_derivative, self.derivative)\n",
        "    #   if arg.derivative is None:\n",
        "    #     arg.derivative = d\n",
        "    #   else:\n",
        "    #     arg.derivative = _add(arg.derivative, d)\n",
        "\n",
        "    #   arg.backward()\n",
        "\n",
        "    # self.derivative = Tensor(1)\n",
        "\n",
        "    self.derivative = Tensor(1)\n",
        "\n",
        "    dfs(self, self.derivative)\n",
        "\n",
        "Tensor(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _add(a: Tensor, b: Tensor):\n",
        "  ret = Tensor(a.value + b.value)\n",
        "  ret.args = (a, b)\n",
        "  ret.local_derivatives = (Tensor(1), Tensor(1))\n",
        "  return ret\n",
        "\n",
        "def _sub(a: Tensor, b: Tensor):\n",
        "  ret = Tensor(a.value - b.value)\n",
        "  ret.args = (a, b)\n",
        "  ret.local_derivatives = (Tensor(1), Tensor(-1))\n",
        "  return ret\n",
        "\n",
        "def _mul(a: Tensor, b: Tensor):\n",
        "  ret = Tensor(a.value * b.value)\n",
        "  ret.args = (a, b)\n",
        "  ret.local_derivatives = (Tensor(b.value), Tensor(a.value))\n",
        "  return ret\n",
        "\n",
        "assert _add(Tensor(1), Tensor(2)).value == 3\n",
        "assert _sub(Tensor(1), Tensor(2)).value == -1\n",
        "assert _mul(Tensor(1), Tensor(2)).value == 2"
      ],
      "metadata": {
        "id": "2_yiZt1vWAzp"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DFS to back propagate\n",
        "# 1. for a node, its derivative equals the sum of derivatives along all paths that back-prop to it\n",
        "# 2. because of the chain rule of deriviate, a node's derivate on that single path equals the multiplying of deriviates of its descend node * local_deriviate\n",
        "def dfs(t: \"Tensor\", curr_derivative: float) -> None:\n",
        "  if not t.args:\n",
        "    return\n",
        "\n",
        "  for arg, local_der in zip(t.args, t.local_derivatives):\n",
        "    derivative = _mul(local_der, curr_derivative)\n",
        "    if arg.derivative is None:\n",
        "      arg.derivative = derivative\n",
        "    else:\n",
        "      arg.derivative = _add(arg.derivative, derivative)\n",
        "\n",
        "    dfs(arg, derivative)"
      ],
      "metadata": {
        "id": "rpwDDZvbG-QL"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit Test\n",
        "add_tensor = _add(Tensor(1), Tensor(2))\n",
        "assert add_tensor.value == 3\n",
        "assert add_tensor.args[0].value == 1\n",
        "assert add_tensor.args[1].value == 2\n",
        "assert add_tensor.local_derivatives[0].value == 1\n",
        "assert add_tensor.local_derivatives[1].value == 1"
      ],
      "metadata": {
        "id": "Agz6YgE_qpR0"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit test back prop\n",
        "#\n",
        "# L = a + b * c\n",
        "a = Tensor(1)\n",
        "b = Tensor(2)\n",
        "c = Tensor(3)\n",
        "\n",
        "L = _add(a, _mul(b, c))\n",
        "L.backward()\n",
        "\n",
        "assert a.derivative.value == 1\n",
        "assert b.derivative.value == 3\n",
        "assert c.derivative.value == 2"
      ],
      "metadata": {
        "id": "Po0G0NRHuz_H"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit test back prop\n",
        "#\n",
        "# L = a + b * (a + c)\n",
        "a = Tensor(1)\n",
        "b = Tensor(2)\n",
        "c = Tensor(3)\n",
        "\n",
        "L = _add(a, _mul(b, _add(a, c)))\n",
        "L.backward()\n",
        "\n",
        "assert a.derivative.value == 1 + 2\n",
        "assert b.derivative.value == 1 + 3\n",
        "assert c.derivative.value == 2"
      ],
      "metadata": {
        "id": "dVnVNO7dwuv5"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit test back prop\n",
        "#\n",
        "# L = d * F\n",
        "# F = H + G\n",
        "# H = a * I\n",
        "# I = a + b\n",
        "# G = c * H\n",
        "#\n",
        "# L = d*a**2 + abd + a**2cd + abcd\n",
        "# dL/da = 2ad + bd + 2acd + bcd\n",
        "# dL/db = ad + acd\n",
        "# dL/dc = a**2d + abd\n",
        "# dL/dd = a**2 + ab + a**2c+abc\n",
        "\n",
        "a = Tensor(1)\n",
        "b = Tensor(2)\n",
        "c = Tensor(3)\n",
        "d = Tensor(4)\n",
        "\n",
        "I = _add(a, b)\n",
        "H = _mul(a, I)\n",
        "G = _mul(c, H)\n",
        "F = _add(H, G)\n",
        "L = _mul(d, F)\n",
        "\n",
        "L.backward()\n",
        "\n",
        "assert L.value == 4*(1**2) + 1*2*4 + (1**2)*3*4 + 1*2*3*4\n",
        "assert a.derivative.value == (2*1*4 + 2*4 + 2*1*3*4 + 2*3*4), f\"{a.derivative=}\""
      ],
      "metadata": {
        "id": "-EHwrvs309mt"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit test back-prop\n",
        "#\n",
        "# Example in https://bclarkson-code.com/posts/llm-from-scratch-scalar-autograd/post.html\n",
        "\n",
        "y = Tensor(1)\n",
        "m = Tensor(2)\n",
        "x = Tensor(3)\n",
        "c = Tensor(4)\n",
        "\n",
        "left = _sub(y, _add(_mul(m, x), c))\n",
        "right = _sub(y, _add(_mul(m, x), c))\n",
        "\n",
        "L = _mul(left, right)\n",
        "L.backward()\n",
        "\n",
        "assert x.derivative.value == 36"
      ],
      "metadata": {
        "id": "ZqDXxYkp3hp0"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}